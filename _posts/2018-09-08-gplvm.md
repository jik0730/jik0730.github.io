---
title: "Gaussian Process Latent Variable Model (GPLVM)"
categories: ML GP
tags: GPLVM dimension-reduction non-linear kernel
---
The standard application of Gaussian Process (GP) models is to supervised learning tasks where inputs and outputs are given in training time. Both of regression and classification problems can be solved via GP models. GP models in unsupervised learning tasks are not trivial. Gaussian Process Latent Variable Model (GPLVM) mainly arises from PPCA. We recall that PPCA is a typical latent variable model with linear mapping and GPLVM is natural extension of PPCA by allowing non-linear mapping from latent variables to the observed variables.

### PPCA

We put prior distribution on the latent variables $$X$$ with i.i.d. assumption and marginalize out $$X$$ to have likelihood of the data. Then the parameters $$W$$ are estimated by MLE. The posterior distribution of $$X$$ can also be derived through *Bayes' rule* by utilizing conjugate prior of $$X$$ (normal-normal).

### Dual PPCA

Alternatively we can put prior distribution on the parameters $$W$$ and marginalize out $$W$$ to have likelihood of the data (treating $$X$$ to be parameters to estimate).

We are given the data $$Y=\{y_1, \cdots, y_N\}$$ where $$y_n \in \mathbb{R}^D$$ and the latent variables $$X=\{x_1, \cdots, x_N\}$$ where $$x_n \in \mathbb{R}^Q$$ with the linear relationship $$Y=XW+\epsilon$$ where $$\epsilon \sim \mathcal{N}(0, \sigma^2\mathcal{I})$$.

We assume $$W=[w_1, \cdots, w_D]$$ to be factorized by column-wise, meaning to individual columns to be independent and put same prior distribution on each column vector of $$W$$ as $$w_d \sim \mathcal{N}(0, \mathcal{I})$$. Then the likelihood over $$Y$$ would be

$$
\begin{align*}
p(Y|X)&=\int p(Y,W|X) \, dW \\
&= \int \prod_{d=1}^D p(y_d, w_d | X) \, dw_1 \cdots dw_D \\
&= \prod_{d=1}^D p(y_d | X) \, .
\end{align*}
$$


Equations above can easily be verified by conditional independences using assumptions we made. Also, $$p(y_d|X)=\mathcal{N}(0, XX^T + \sigma^2\mathcal{I})$$ holds since $$y_d = Xw_d + \epsilon$$ where we used linear Gaussian property. Then the estimation of $$X$$ is calculated through MLE similar to PPCA.

### GPLVM

If we see closely on $ p(y_d|X)=\mathcal{N}(0, XX^T + \sigma^2\mathcal{I}) $, we can notice that the form is actually GP with linear covariance function. Furthermore, $ p(Y|X) $ is a product of $ D $ independent GP. We can naturally extend linear mapping to non-linear mapping which is novel idea of *kernel trick* when using GP. To fully utilize the property of GP as *an universal approximator*, we can put RBF kernel function to the covariance function of $ p(y_d|X) $.

$$
k(x_n, x_m) = v^2 \exp(-\frac{||x_n-x_m||^2}{2l^2})
$$

Since the likelihood function with RBF kernel is higly non-linear, we optimize $$X$$ as well as other hyper-parameters through gradient based optimization.
