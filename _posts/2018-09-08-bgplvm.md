---
title: "Bayesian Gaussian Process Latent Variable Model (BGPLVM)"
categories: ML GP
tags: BGPLVM GPLVM Bayesian dimension-reduction kernel variational-inference sparse-GP
---
input variables is intractable in GPLVM because of non-linearity in covariance matrix. Instead, techniques of variational inference and augmenting inducing variables help to calculate lower bound of marginal likelihood of given data.

### Settings

We denote $Y \in \mathbb{R}^{N \times D}$ as a set of observed variables and $X \in \mathbb{R}^{N \times Q}$ as a set of latent variables where $Q \ll D$ for dimension reduction. 

### GPLVM

The likelihood of a set of observed variables is as follow.
$$
p(Y|X)=\prod_{d=1}^D \mathcal{N}(y_d | 0, K_{NN}+\beta^{-1}\mathcal{I})
$$
The values of $X$ are deterministically determined by optimizing through gradient based methods. This formulation can lead serious overfitting when the number of data points is small.

### B-GPLVM

B-GPLVM puts prior distribution on latent variables and marginalizing out the variables to prevent overfitting. As we mentioned earlier this is intractable so that we optimize lower bound of marginal likelihood of data instead through variational inference.

First we assume a prior to $X$ as $p(X)=\prod_{n=1}^N \mathcal{N}(x_n|0, \mathcal{I})$. We define the variational distribution $q(X)=\prod_{n=1}^N \mathcal{N}(x_n|\mu_n, S_n)$ to approximate $p(X|Y)$.
$$
\begin{align*}
KL[q(X)||p(X|Y)] &= \int q(X) \log \frac{q(X)}{p(X|Y)} \, dX \\
&= \int q(X) \log \frac{q(X)p(Y)}{p(X,Y)} \, dX \\
&= \log p(Y) + \int q(X) \log \frac{q(X)}{p(X,Y)} \, dX \\
&= \log p(Y) - \int q(X) \log p(Y|X) \, dX + \int q(X) \log \frac{q(X)}{p(X)} \, dX
\end{align*}
$$
Rephrasing,
$$
\begin{align*}
\log p(Y) &= KL[q(X)||p(X|Y)] + \int q(X) \log p(Y|X) \, dX - \int q(X) \log \frac{q(X)}{p(X)} \, dX \\
&= KL[q(X)||p(X|Y)] + F(q) \, .
\end{align*}
$$
This means that $F(q)â$ is a lower bound of the marginal likelihood $\log p(Y)â$ since KL divergence is always positive and by maximizing $F(q)â$ instead of $\log p(Y)â$ we make the approximated variational distribution $q(X)â$ to be closer to the true posterior distribution $p(X|Y)â$.
$$
\begin{align*}
F(q) &= \int q(X) \log p(Y|X) \, dX - \int q(X) \log \frac{q(X)}{p(X)} \, dX \\
&= \widetilde F (q) - KL[q(X)||p(X)]
\end{align*}
$$
Since $q(X)$ and $p(X)$ are all Gaussian distributions, KL divergence term in $F(q)$ can be easily computed. However, $\widetilde F (q)$ still contains intractable integration. This term needs to be handled by approximation and can be factorized as follow.
$$
\begin{align*}
\widetilde F (q) &= \int q(X) \log p(Y|X) \, dX \\
&= \sum_{d=1}^D \int q(X) \log p(y_d|X) \, dX = \sum_{d=1}^D \widetilde F_d (q)
\end{align*}
$$
From now on, we introduce the inducing variables technique to approximate $\widetilde F_d (q)â$. Let $u_dâ$ be inducing variables and $Zâ$ be corresponding inducing (pseudo) inputs. The intractable term in $\widetilde F_d (q)â$ is $p(y_d|X)â$ and the term can be computed by marginalizing out latent function variables $f_dâ$ and inducing variables $u_dâ$ as $p(y_d|X)=\int\int p(y_d, f_d, u_d|X) \, df_d \, du_dâ$. Again, we apply variational inference on the variables where we want to marginalize out. We define the variational distribution of the variables to approximate true posterior distribution of the variables as follows. We drop the variable $Zâ$ for simplicity and the variable is treated as variational parameters that we optimize.
$$
\begin{align*}
p(f_d, u_d|y_d, X) &= p(f_d|u_d, y_d, X) \, p(u_d|y_d,X) \\
q(f_d, u_d) &= p(f_d|u_d,X) \, \phi(u_d)
\end{align*}
$$

where we also assume that $u_d$ is a sufficient statistics for $f_d$ and $\phi(\cdot)$ is a variational distribution again to overcome intractability caused by $X$. Then we similarly proceed the procedure of variational inference to get lower bound of $\log p(y_d|X)$. 
$$
\begin{align*}
KL[q(f_d, u_d)||p(f_d,u_d|y_d,X)] &= \int \int q(f_d, u_d) \log \frac{q(f_d, u_d)}{p(f_d,u_d|y_d,X)} \, df_d \, du_d \\
&= \int \int q(f_d, u_d) \log \frac{p(y_d|X)\phi(u_d)}{p(y_d|f_d)p(u_d)} \, df_d \, du_d \\
&= \log p(y_d|X) + \int \phi(u_d) \log \frac{\phi(u_d)}{p(u_d)} \, du_d - \int \int p(f_d|u_d,X)\phi(u_d) \log p(y_d|f_d) \, df_d \, du_d \\
&= \log p(y_d|X) + \int \phi(u_d) \log \frac{\phi(u_d)}{p(u_d)} \, du_d - S(\phi)
\end{align*}
$$
$S(\phi)$ in above equations is tricky to evaluate.
$$
\begin{align*}
S(\phi) &= \int \int p(f_d|u_d,X)\phi(u_d) \log p(y_d|f_d) \, df_d \, du_d \\
&= \int \int p(f_d|u_d,X)\phi(u_d) \log \mathcal{N}(y_d|f_d, \beta^{-1}\mathcal{I}) \, df_d \, du_d \\
&= \int -\frac{1}{2} \log \Big(\frac{2\pi}{\beta}\Big)^N \int p(f_d|u_d,X) \Big[ - \frac{\beta}{2} (y_d-f_d)^T(y_d-f_d) \Big] \, df_d \phi(u_d) \, du_d \\
&= \int \Bigg[ -\frac{1}{2} \log \Big(\frac{2\pi}{\beta}\Big)^N - \frac{\beta}{2} \Big( y_d^Ty_d - 2\alpha_d^Ty_d + \alpha_d^T\alpha_d + \mathrm{Tr}\big(\Sigma_\alpha\big) \Big) \Bigg] \phi(u_d) \, du_d \\
&= \int \phi(u_d) \log \mathcal{N}(y_d|\alpha_d, \beta^{-1}\mathcal{I}) \, du_d - \frac{\beta}{2} \mathrm{Tr} \big(\Sigma_\alpha\big)
\end{align*}
$$
where $p(f_d|u_d,X,Z)=\mathcal{N}(f_d|\alpha_d, \Sigma_\alpha)$, $\alpha_d=K_{NM}K_{MM}^{-1}u_d$ and $\Sigma_\alpha=K_{NN} - K_{NM}K_{MM}^{-1}K_{MN}$. Finally in the end,
$$
\log p(y_d|X) = KL[q(f_d, u_d)||p(f_d,u_d|y_d,X)] + \int \phi(u_d) \log \frac{p(u_d)\mathcal{N}(y_d|\alpha_d, \beta^{-1}\mathcal{I})}{\phi(u_d)} \, du_d - \frac{\beta}{2} \mathrm{Tr} \big(K_{NN} - K_{NM}K_{MM}^{-1}K_{MN}\big) \, .
$$
Then, we return to get lower bound of $\widetilde F_d (q)$.
$$
\begin{align*}
\widetilde F_d (q) &\geq \int q(X) \Bigg[ \int \phi(u_d) \log \frac{p(u_d)\mathcal{N}(y_d|\alpha_d, \beta^{-1}\mathcal{I})}{\phi(u_d)} \, du_d - \frac{\beta}{2} \mathrm{Tr} \big(K_{NN}\big) + \frac{\beta}{2} \mathrm{Tr} \big(K_{NM}K_{MM}^{-1}K_{MN}\big) \Bigg] \, dX \\
&\geq \int \phi(u_d) \Bigg[ \langle \log \mathcal{N}(y_d|\alpha_d, \beta^{-1}\mathcal{I}) \rangle_{q(X)} + \log \frac{p(u_d)}{\phi(u_d)} \, \Bigg] du_d - \frac{\beta}{2} \mathrm{Tr} \big(\langle K_{NN} \rangle_{q(X)} \big) + \frac{\beta}{2} \mathrm{Tr} \big(K_{MM}^{-1} \langle K_{MN}K_{NM} \rangle_{q(X)} \big)
\end{align*}
$$
where we change order of integrand since two distributions are independent. We further make tight for the lower bound of $\widetilde F_d(q)$ by applying *Jensen's inequality* on $\log$ function. By applying *Jensen's inequality*, the variational distribution $\phi(u_d)$ is optimally eliminated out as followings.
$$
\widetilde F_d (q) \geq \log \Bigg[ \int \exp \Big( \langle \log \mathcal{N}(y_d|\alpha_d, \beta^{-1}\mathcal{I}) \rangle_{q(X)} \Big) p(u_d) \, du_d \Bigg] - \frac{\beta}{2} \mathrm{Tr} \big(\langle K_{NN} \rangle_{q(X)} \big) + \frac{\beta}{2} \mathrm{Tr} \big(K_{MM}^{-1} \langle K_{MN}K_{NM} \rangle_{q(X)} \big)
$$

This quantity boils down to compute three quantities.
$$
\begin{align*}
\psi_0 &= \mathrm{Tr}(\langle K_{NN} \rangle_{q(X)}) \\
\Psi_1 &= \langle K_{NM} \rangle_{q(X)} \\
\Psi_2 &= \langle K_{MN}K_{NM} \rangle_{q(X)}
\end{align*}
$$
The Second and third terms in the lower bound are trivial and the first one is computed as follows.
$$
\begin{align*}
\langle \log \mathcal{N}(y_d|\alpha_d, \beta^{-1}\mathcal{I}) \rangle_{q(X)} &= \int q(X) \Bigg \{ -\frac{N}{2} \log \Big( \frac{2\pi}{\beta} \Big) -\frac{\beta}{2} \big( y_d^Ty_d - 2y_d^T\alpha_d + \alpha_d^T\alpha_d \big) \Bigg \} \, dX \\
&= -\frac{N}{2} \log \Big( \frac{2\pi}{\beta} \Big) -\frac{\beta}{2} \big( y_d^Ty_d - 2y_d^T \Psi_1 K_{MM}^{-1}u_d + u_d^T K_{MM}^{-1} \Psi_2 K_{MM}^{-1} u_d \big)
\end{align*}
$$
The term above is a quadratic function of $u_dâ$ and the lower bound is just a standard Gaussian integral, which mean it is computed analytically. In the end after tedious calculation with matrix inversion lemma and matrix determinant lemma, the final closed-form lower bound is:
$$
\widetilde F_d (q) \geq \log \Bigg[ \frac{(\beta)^{\frac{N}{2}}|K_{MM}|^{\frac{1}{2}}}{(2\pi)^{\frac{N}{2}}|\beta \Psi_2 + K_{MM}|^{\frac{1}{2}}} \exp \Big( -\frac{1}{2}y_d^TWy_d \Big) \Bigg] - \frac{\beta \psi_0}{2} + \frac{\beta}{2} \mathrm{Tr} \big(K_{MM}^{-1} \Psi_2 \big)
$$
where $W = \beta \mathcal{I} - \beta^2 \Psi_1 (\beta \Psi_2 + K_{MM})^{-1}\Psi_1^T$.
